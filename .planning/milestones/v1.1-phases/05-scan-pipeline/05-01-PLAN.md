---
phase: 05-scan-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/screenshotone.ts
  - lib/cache.ts
  - app/api/scan/url/route.ts
  - app/api/scan/photo/route.ts
  - package.json
autonomous: true
requirements: [SCAN-02, SCAN-03, SCAN-05]

user_setup:
  - service: screenshotone
    why: "Screenshot API for SPA menu text extraction (eazee-link.com is a JS SPA)"
    env_vars:
      - name: SCREENSHOTONE_ACCESS_KEY
        source: "https://screenshotone.com -> Dashboard -> API Access -> Access key"
      - name: SCREENSHOTONE_SECRET_KEY
        source: "https://screenshotone.com -> Dashboard -> API Access -> Secret key"
    dashboard_config:
      - task: "Create a free account at screenshotone.com (100 screenshots/month free tier)"
        location: "https://screenshotone.com"

must_haves:
  truths:
    - "POST /api/scan/url with a URL returns a { menuId } JSON response after calling Screenshotone + LLM"
    - "POST /api/scan/photo with a FormData image returns a { menuId } JSON response after calling GPT-4o Vision"
    - "A second POST /api/scan/url with the same URL returns instantly from cache (no Screenshotone or LLM call)"
    - "Photo path uses pre-parsed vision result — getOrParseMenu does NOT call parseDishesFromMenu a second time"
  artifacts:
    - path: "lib/screenshotone.ts"
      provides: "Server-only Screenshotone wrapper for SPA text extraction"
      contains: "extractMenuText"
    - path: "app/api/scan/url/route.ts"
      provides: "URL scan Route Handler"
      exports: ["POST"]
    - path: "app/api/scan/photo/route.ts"
      provides: "Photo OCR Route Handler"
      exports: ["POST"]
    - path: "lib/cache.ts"
      provides: "Updated getOrParseMenu with optional preParseResult parameter"
      contains: "preParseResult"
  key_links:
    - from: "app/api/scan/url/route.ts"
      to: "lib/screenshotone.ts"
      via: "extractMenuText import"
      pattern: "import.*extractMenuText.*from.*screenshotone"
    - from: "app/api/scan/url/route.ts"
      to: "lib/cache.ts"
      via: "getOrParseMenu import"
      pattern: "import.*getOrParseMenu.*from.*cache"
    - from: "app/api/scan/photo/route.ts"
      to: "lib/cache.ts"
      via: "getOrParseMenu with preParseResult"
      pattern: "getOrParseMenu.*preParseResult"
---

<objective>
Build the server-side scan pipeline: Screenshotone text extraction wrapper, URL and photo API Route Handlers, and adapt the cache layer for pre-parsed photo results.

Purpose: Phase 5 client components will POST to these endpoints. The URL path extracts text from JavaScript SPAs via Screenshotone (format=markdown), then feeds it through getOrParseMenu. The photo path uses GPT-4o Vision directly, then stores the pre-parsed result via getOrParseMenu (skipping the redundant LLM call). Cache layer (SCAN-05) already works from Phase 4 — this plan wires the entry points and adapts getOrParseMenu for the photo pre-parse pattern.

Output: Two working Route Handlers (POST /api/scan/url, POST /api/scan/photo) and a Screenshotone wrapper, all testable via curl.
</objective>

<execution_context>
@/Users/ekitcho/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ekitcho/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-scan-pipeline/05-RESEARCH.md
@.planning/phases/04-infrastructure-foundation/04-02-SUMMARY.md
@lib/cache.ts
@lib/openai.ts
@lib/types/llm.ts
@lib/types/menu.ts
@lib/types/config.ts
@lib/supabase-admin.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install dependencies and create Screenshotone wrapper</name>
  <files>
    package.json
    lib/screenshotone.ts
  </files>
  <action>
Install the three new dependencies:
```bash
npm install screenshotone-api-sdk@1.1.21 qr-scanner@1.4.2 browser-image-compression@2.0.2
```
(qr-scanner and browser-image-compression are client-side — installed here so Plan 02 doesn't need a separate install step.)

Create `lib/screenshotone.ts` — a server-only module that wraps Screenshotone SDK for SPA text extraction:
- `import 'server-only'` at top
- Import `screenshotone-api-sdk` (use `import * as screenshotone from 'screenshotone-api-sdk'`)
- Create a `Client` instance from `SCREENSHOTONE_ACCESS_KEY` and `SCREENSHOTONE_SECRET_KEY` env vars
- Export `extractMenuText(url: string): Promise<string>` function:
  - Use `screenshotone.TakeOptions.url(url)` with `.format('markdown')`, `.waitUntil(['networkidle2'])`, `.fullPage(true)`, `.delay(2)`
  - If `.format('markdown')` is not available in the SDK type definitions, build the URL manually by appending `&format=markdown` to the generated signed URL, or use an alternative method — check the SDK types at implementation time
  - Generate a signed URL via `client.generateSignedTakeUrl(options)`
  - `fetch()` the signed URL
  - Validate response: if `!response.ok`, throw with status code and text
  - Validate content: if `text.trim().length < 50`, throw an error indicating the URL returned empty content
  - Return the markdown text string
- Add `export const maxDuration = 60` to support Vercel Pro plan timeout (Screenshotone + LLM can take 6-15s total)
  </action>
  <verify>
`npx tsc --noEmit` passes with no errors on the new files. `npm run build` succeeds (server-only import works correctly, no client-side import of screenshotone).
  </verify>
  <done>
`lib/screenshotone.ts` exports `extractMenuText` function; `screenshotone-api-sdk`, `qr-scanner`, and `browser-image-compression` are in package.json dependencies; TypeScript compiles without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create API Route Handlers and adapt getOrParseMenu for photo pre-parse</name>
  <files>
    lib/cache.ts
    app/api/scan/url/route.ts
    app/api/scan/photo/route.ts
  </files>
  <action>
**Step A: Adapt `getOrParseMenu` in `lib/cache.ts`**

Add an optional `preParseResult` parameter to `getOrParseMenu`:

```typescript
export async function getOrParseMenu(
  url: string,
  sourceType: 'url' | 'photo' | 'qr',
  rawText: string,
  preParseResult?: { dishes: DishResponse[] }
): Promise<MenuWithItems> {
```

Add `import type { DishResponse } from './types/llm';` at the top.

In the cache miss path, change the LLM call to use preParseResult if provided:
```typescript
// Step 4: Cache MISS — use pre-parsed result or call LLM
const parsed = preParseResult ?? await parseDishesFromMenu(rawText, config.llm_model);
```

This allows the photo Route Handler to pass its Vision OCR result directly, avoiding a redundant LLM call. The rest of the function (insert menu + items, return MenuWithItems) stays unchanged.

**Step B: Create `app/api/scan/url/route.ts`**

- `import 'server-only'` at top
- Import `NextRequest`, `NextResponse` from `next/server`
- Import `getOrParseMenu` from `@/lib/cache`
- Import `extractMenuText` from `@/lib/screenshotone`
- Export `const maxDuration = 60` (Vercel Pro timeout)
- Export `async function POST(req: NextRequest)`:
  1. Parse JSON body: `const { url } = await req.json()`
  2. Validate: if `!url || typeof url !== 'string'`, return 400 with `{ error: 'Missing or invalid url' }`
  3. Validate URL format: basic check with `new URL(url)` in a try/catch — return 400 if invalid
  4. Try/catch the main pipeline:
     - `const menuText = await extractMenuText(url)`
     - `const menu = await getOrParseMenu(url, 'url', menuText)`
     - Return `NextResponse.json({ menuId: menu.id })`
  5. Catch: return 500 with `{ error: 'Failed to parse menu. Please try again.' }` and `console.error` the full error

**Step C: Create `app/api/scan/photo/route.ts`**

- `import 'server-only'` at top
- Import `NextRequest`, `NextResponse` from `next/server`
- Import `generateText`, `Output`, `NoObjectGeneratedError` from `ai`
- Import `openai` from `@ai-sdk/openai`
- Import `z` from `zod`
- Import `dishResponseSchema` from `@/lib/types/llm`
- Import `getOrParseMenu`, `getAdminConfig` from `@/lib/cache`
- Import the `MENU_PARSE_SYSTEM_PROMPT` from `@/lib/openai` — NOTE: this prompt is currently a module-level `const`, not exported. You will need to export it from `lib/openai.ts` first. Add `export` to the existing `const MENU_PARSE_SYSTEM_PROMPT` declaration in `lib/openai.ts`.
- Export `const maxDuration = 60`
- Export `async function POST(req: NextRequest)`:
  1. `const formData = await req.formData()`
  2. `const file = formData.get('image') as File | null`
  3. If `!file`, return 400 `{ error: 'Missing image file' }`
  4. `const arrayBuffer = await file.arrayBuffer()`
  5. `const config = await getAdminConfig()`
  6. Try/catch the vision pipeline:
     - Call `generateText` with:
       - `model: openai(config.llm_model)` (admin-configurable model)
       - `output: Output.object({ schema: z.object({ dishes: z.array(dishResponseSchema) }) })`
       - `system: MENU_PARSE_SYSTEM_PROMPT`
       - `messages: [{ role: 'user', content: [{ type: 'image', image: arrayBuffer, mediaType: file.type as 'image/jpeg' | 'image/png' | 'image/webp' }, { type: 'text', text: 'This is a photo of a restaurant menu. Extract all dishes with their names, descriptions, prices, and allergens. Process every visible dish.' }] }]`
       - `maxRetries: 2`
     - Destructure `{ experimental_output: output }`
     - Create synthetic photo URL: `const photoUrl = \`photo:\${Date.now()}\``
     - Call `getOrParseMenu(photoUrl, 'photo', '[photo upload]', output)` — pass the vision result as `preParseResult`
     - Return `NextResponse.json({ menuId: menu.id })`
  7. Catch: if `NoObjectGeneratedError`, log `error.text` and `error.cause`; return 500 with `{ error: 'Could not extract dishes from photo. Try a clearer image.' }`; otherwise rethrow

Create necessary directories: `app/api/scan/url/` and `app/api/scan/photo/`.
  </action>
  <verify>
1. `npx tsc --noEmit` passes
2. `npm run build` succeeds
3. `MENU_PARSE_SYSTEM_PROMPT` is exported from `lib/openai.ts`
4. `getOrParseMenu` signature now accepts optional 4th parameter `preParseResult`
5. Both route files export a `POST` function and `maxDuration`
  </verify>
  <done>
POST /api/scan/url accepts `{ url }` JSON and returns `{ menuId }` after Screenshotone extraction + LLM parse + cache write. POST /api/scan/photo accepts FormData with `image` field and returns `{ menuId }` after GPT-4o Vision parse + cache write (no redundant LLM call). Both routes handle errors with user-friendly messages. getOrParseMenu supports the pre-parse pattern for photo uploads.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` passes with zero errors
2. `npx tsc --noEmit` reports no type errors
3. `lib/screenshotone.ts` exists with `extractMenuText` export and `import 'server-only'`
4. `app/api/scan/url/route.ts` exists with `POST` export
5. `app/api/scan/photo/route.ts` exists with `POST` export
6. `lib/cache.ts` has `preParseResult` as optional 4th param on `getOrParseMenu`
7. `lib/openai.ts` exports `MENU_PARSE_SYSTEM_PROMPT`
</verification>

<success_criteria>
- Both Route Handlers compile and are accessible as Next.js API routes
- URL scan path: Screenshotone extracts text -> getOrParseMenu caches and returns menuId
- Photo scan path: Vision OCR produces dishes -> getOrParseMenu stores pre-parsed result (no double LLM call)
- Cache hit path: second call with same URL skips Screenshotone and LLM entirely
- All server-only guards in place (no client-side import possible)
</success_criteria>

<output>
After completion, create `.planning/phases/05-scan-pipeline/05-01-SUMMARY.md`
</output>
